---
title: "Sentiment Analysis"
description: |
   Sentiment Analysis.
author:
  - name: Sathvik Thogaru
    url: https://thogarusathvik.github.io/Text-as-Data-Project/
date: 03-25-2022
output:
  distill::distill_article:
    self_contained: false
  html_document:
    number_sections: true
  pdf_document:
    number_sections: true
---


```{css, echo = FALSE}
pre code {
  white-space: pre-wrap;
}

```


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<style>
body {
text-align: justify}
</style>



```{r}
library(devtools)
#devtools::install_github("kbenoit/quanteda.dictionaries") 
library(quanteda.dictionaries)
#devtools::install_github("quanteda/quanteda.sentiment")
library(quanteda.sentiment)
```


```{r}
library(readr)
library(lubridate)
library(skimr)
library(ggplot2)
library(stringr)
library(dplyr)
library(quanteda)
library(quanteda.textplots)
library(tidytext)

twitter_data <- read_csv("../../twitter_data.csv")
```

# summary of the data

```{r}
skim(twitter_data)
```

# tweets before cleaning

```{r}
twitter_data$text[1:10]
```


varibales that might be useful in the future analysis : 
created_at, screen_name, text, source, is_retweet, retweet_count, hashtags, symbols, mentions_screen_name, quoted_text, quoted_created_at, quoted_screen_name, quoted_name, quoted_followers_count,
quoted_location, quoted_description, retweet_text, retweet_created_at, retweet_screen_name, retweet_name,         retweet_followers_count, retweet_friends_count, retweet_statuses_count, retweet_location, retweet_description, place_name,
place_type, place_full_name, country, country_code, geo_coords, coords_coords, bbox_coords, name, location, description,

 
# cleaning tweets

```{r}
clean <- function (text) {
  str_remove_all(text," ?(f|ht)(tp)(s?)(://)(.*)[.|/](.*)") %>%
                            # Remove mentions
                            str_remove_all("@[[:alnum:]_]*") %>%
                            # Remove hash tags
                            str_remove_all("#[[:alnum:]_]+") %>%
                            # Replace "&" character reference with "and"
                            str_replace_all("&amp;", "and") %>%
                            # Remove punctuation, using a standard character class
                            str_remove_all("[[:punct:]]") %>%
                            # remove digits
                            str_remove_all("[[:digit:]]") %>%
                            # Remove "RT: " from beginning of retweets
                            str_remove_all("^RT:? ") %>%
                            # Replace any newline characters with a space
                            str_replace_all("\\\n|\\\r", " ") %>%
                            # remove strings like "<U+0001F9F5>"
                            str_remove_all("<.*?>") %>% 
                            # Make everything lowercase
                            str_to_lower() %>%
                            # Remove any trailing white space around the text and inside a string
                            str_squish()
}
```

```{r}
twitter_data$text <- clean(twitter_data$text)
twitter_data$retweet_text <- clean(twitter_data$retweet_text)
```

looking at the cleaned tweets

```{r}
twitter_data$text[1:10]
```

converting the character vectors, `text`, `retweet_text` to a single corpus

```{r}
text <- corpus(c(twitter_data$text, twitter_data$retweet_text))
text <- dfm(tokens(text, remove_punct=TRUE, remove_numbers = TRUE) %>%
             tokens_select(pattern=stopwords("en"),
                            selection="remove"))
```

finding the word frequencies in the documents/ tweets

```{r}
word_counts <- as.data.frame(sort(colSums(text),dec=T))
colnames(word_counts) <- c("Frequency")
word_counts$word <- row.names(word_counts)
word_counts$Rank <- c(1:ncol(text))
head(word_counts)
```
counting the words that have frequency leass than 50

```{r}
sum(word_counts$Frequency < 50)
```
 looking at how word frequencies are distributed
 
```{r}
ggplot(word_counts, mapping = aes(x = Rank, y = Frequency)) + 
  geom_point() +
  labs(title = "Zipf's Law", x = "Rank", y = "Frequency") + 
  theme_bw()
```


Having seen what we are working with here, we might start to think that our matrix still contains too many uninformative or very rare terms. We can trim our DFM in two different ways related to feature frequencies using dfm_trim().


```{r}
# let's create a nicer dfm by limiting to words that appear frequently and are in more than 30% of tweets
#text_dfm <- dfm_trim(text, min_termfreq = 50)
text_dfm <- dfm_trim(text, min_termfreq = .3, docfreq_type = "prop")

# create fcm from dfm
text_fcm <- fcm(text_dfm)

# check the dimensions (i.e., the number of rows and the number of columnns)
# of the matrix we created
dim(text_fcm)
head(text_fcm, 10)
```

```{r}
# pull the top features
myFeatures <- names(topfeatures(text_fcm, 30))

# retain only those top features as part of our matrix
even_text_fcm <- fcm_select(text_fcm, pattern = myFeatures, selection = "keep")

# check dimensions
dim(even_text_fcm)

# compute size weight for vertices in network
size <- log(colSums(even_text_fcm))

# create plot
textplot_network(even_text_fcm, vertex_size = size / max(size) * 3)
```



The function get_sentiments() allows us to get specific sentiment lexicons with the appropriate measures for each one. This is a function in tidytext package

This dataset nrc was published in Saif M. Mohammad and Peter Turney. (2013), ``Crowdsourcing a Word-Emotion Association Lexicon.'' Computational Intelligence, 29(3): 436-465.

```{r}
nrc <- get_sentiments("nrc")
```

this dataset afinn was published by Finn Årup Nielsen (http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=6010)

```{r}
afinn <- get_sentiments("afinn")
```

this dataset bing was published by Bing Liu and collaborators,https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html

```{r}
bing <- get_sentiments("bing")

```


sentiment analysis

The nrc lexicon categorizes words in a binary fashion (“yes”/“no”) into categories of positive, negative, anger, anticipation, disgust, fear, joy, sadness, surprise, and trust.

```{r}
(nrc <- word_counts %>%
  inner_join(nrc, by = "word") %>% 
  group_by(sentiment) %>% 
  summarise(frequency = n()))

  ggplot(nrc, aes(x = sentiment, y = frequency/sum(frequency), fill = sentiment))+
  geom_bar(stat='identity')+
  labs(y = "percentage of emotion")+
  theme_classic()
```


The bing lexicon categorizes words in a binary fashion into positive and negative categories.

```{r}
word_counts %>%
  inner_join(bing, by = "word") %>% 
  group_by(sentiment) %>% 
  summarise(frequency = n()) 
  
```

The AFINN lexicon assigns words with a score that runs between -5 and 5, with negative scores indicating negative sentiment and positive scores indicating positive sentiment.

```{r}
word_counts %>%
  inner_join(afinn, by = "word") %>% 
  group_by(value) %>% 
  summarise(frequency = n()) %>% 
  arrange(desc(frequency))
  
```



most common positive and negative words

```{r}
head(bing_word_counts <- word_counts %>%
  inner_join(get_sentiments("bing")) %>%
   select(word, Frequency, sentiment), 30)
```

```{r}
head(nrc_word_counts <- word_counts %>%
  inner_join(get_sentiments("nrc")) %>%
   filter(sentiment %in% c("positve", "negative")) %>% 
   select(word, Frequency, sentiment), 30)
```




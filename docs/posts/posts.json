[
  {
    "path": "posts/fouth_post/",
    "title": "Sentiment Analysis",
    "description": "Sentiment Analysis.",
    "author": [
      {
        "name": "Sathvik Thogaru",
        "url": "https://thogarusathvik.github.io/Text-as-Data-Project/"
      }
    ],
    "date": "2022-03-25",
    "categories": [],
    "contents": "\r\n\r\n\r\npre code {\r\n  white-space: pre-wrap;\r\n}\r\n\r\n\r\n\r\nbody {\r\ntext-align: justify}\r\n\r\n\r\nlibrary(devtools)\r\n#devtools::install_github(\"kbenoit/quanteda.dictionaries\") \r\nlibrary(quanteda.dictionaries)\r\n#devtools::install_github(\"quanteda/quanteda.sentiment\")\r\nlibrary(quanteda.sentiment)\r\n\r\n\r\n\r\n\r\n\r\nlibrary(readr)\r\nlibrary(lubridate)\r\nlibrary(skimr)\r\nlibrary(ggplot2)\r\nlibrary(stringr)\r\nlibrary(dplyr)\r\nlibrary(quanteda)\r\nlibrary(quanteda.textplots)\r\nlibrary(tidytext)\r\n\r\ntwitter_data <- read_csv(\"../../twitter_data.csv\")\r\n\r\n\r\n\r\nsummary of the data\r\n\r\n\r\nskim(twitter_data)\r\n\r\n\r\nTable 1: Data summary\r\nName\r\ntwitter_data\r\nNumber of rows\r\n17943\r\nNumber of columns\r\n90\r\n_______________________\r\n\r\nColumn type frequency:\r\n\r\ncharacter\r\n58\r\nlogical\r\n10\r\nnumeric\r\n18\r\nPOSIXct\r\n4\r\n________________________\r\n\r\nGroup variables\r\nNone\r\nVariable type: character\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmin\r\nmax\r\nempty\r\nn_unique\r\nwhitespace\r\nuser_id\r\n0\r\n1.00\r\n6\r\n20\r\n0\r\n13493\r\n0\r\nstatus_id\r\n0\r\n1.00\r\n20\r\n20\r\n0\r\n17943\r\n0\r\nscreen_name\r\n0\r\n1.00\r\n3\r\n15\r\n0\r\n13493\r\n0\r\ntext\r\n0\r\n1.00\r\n16\r\n918\r\n0\r\n6942\r\n0\r\nsource\r\n0\r\n1.00\r\n4\r\n76\r\n0\r\n355\r\n0\r\nreply_to_status_id\r\n17082\r\n0.05\r\n20\r\n20\r\n0\r\n846\r\n0\r\nreply_to_user_id\r\n16971\r\n0.05\r\n7\r\n20\r\n0\r\n763\r\n0\r\nreply_to_screen_name\r\n16971\r\n0.05\r\n2\r\n15\r\n0\r\n763\r\n0\r\nhashtags\r\n3071\r\n0.83\r\n2\r\n350\r\n0\r\n4379\r\n0\r\nsymbols\r\n17861\r\n0.00\r\n3\r\n76\r\n0\r\n24\r\n0\r\nurls_url\r\n14431\r\n0.20\r\n8\r\n122\r\n0\r\n2511\r\n0\r\nurls_t.co\r\n14431\r\n0.20\r\n23\r\n119\r\n0\r\n2927\r\n0\r\nurls_expanded_url\r\n14431\r\n0.20\r\n17\r\n621\r\n0\r\n2696\r\n0\r\nmedia_url\r\n14880\r\n0.17\r\n46\r\n87\r\n0\r\n2674\r\n0\r\nmedia_t.co\r\n14880\r\n0.17\r\n23\r\n23\r\n0\r\n2690\r\n0\r\nmedia_expanded_url\r\n14880\r\n0.17\r\n58\r\n70\r\n0\r\n2680\r\n0\r\nmedia_type\r\n14880\r\n0.17\r\n5\r\n5\r\n0\r\n1\r\n0\r\next_media_url\r\n14880\r\n0.17\r\n46\r\n187\r\n0\r\n2674\r\n0\r\next_media_t.co\r\n14880\r\n0.17\r\n23\r\n95\r\n0\r\n2690\r\n0\r\next_media_expanded_url\r\n14880\r\n0.17\r\n58\r\n283\r\n0\r\n2680\r\n0\r\nmentions_user_id\r\n4424\r\n0.75\r\n7\r\n721\r\n0\r\n3302\r\n0\r\nmentions_screen_name\r\n4424\r\n0.75\r\n3\r\n542\r\n0\r\n3302\r\n0\r\nlang\r\n0\r\n1.00\r\n2\r\n2\r\n0\r\n1\r\n0\r\nquoted_status_id\r\n17408\r\n0.03\r\n19\r\n20\r\n0\r\n501\r\n0\r\nquoted_text\r\n17408\r\n0.03\r\n8\r\n1773\r\n0\r\n501\r\n0\r\nquoted_source\r\n17408\r\n0.03\r\n6\r\n25\r\n0\r\n30\r\n0\r\nquoted_user_id\r\n17408\r\n0.03\r\n7\r\n20\r\n0\r\n452\r\n0\r\nquoted_screen_name\r\n17408\r\n0.03\r\n2\r\n15\r\n0\r\n452\r\n0\r\nquoted_name\r\n17408\r\n0.03\r\n1\r\n218\r\n0\r\n452\r\n0\r\nquoted_location\r\n17500\r\n0.02\r\n2\r\n172\r\n0\r\n265\r\n0\r\nquoted_description\r\n17417\r\n0.03\r\n13\r\n514\r\n0\r\n443\r\n0\r\nretweet_status_id\r\n6242\r\n0.65\r\n20\r\n20\r\n0\r\n2737\r\n0\r\nretweet_text\r\n6242\r\n0.65\r\n41\r\n917\r\n0\r\n2734\r\n0\r\nretweet_source\r\n6242\r\n0.65\r\n4\r\n76\r\n0\r\n86\r\n0\r\nretweet_user_id\r\n6242\r\n0.65\r\n7\r\n20\r\n0\r\n1645\r\n0\r\nretweet_screen_name\r\n6242\r\n0.65\r\n3\r\n15\r\n0\r\n1645\r\n0\r\nretweet_name\r\n6242\r\n0.65\r\n1\r\n279\r\n0\r\n1644\r\n0\r\nretweet_location\r\n7540\r\n0.58\r\n2\r\n204\r\n0\r\n875\r\n0\r\nretweet_description\r\n6346\r\n0.65\r\n4\r\n808\r\n0\r\n1589\r\n0\r\nplace_url\r\n17831\r\n0.01\r\n56\r\n56\r\n0\r\n92\r\n0\r\nplace_name\r\n17831\r\n0.01\r\n5\r\n40\r\n0\r\n91\r\n0\r\nplace_full_name\r\n17831\r\n0.01\r\n6\r\n40\r\n0\r\n92\r\n0\r\nplace_type\r\n17831\r\n0.01\r\n3\r\n7\r\n0\r\n4\r\n0\r\ncountry\r\n17831\r\n0.01\r\n5\r\n26\r\n0\r\n17\r\n0\r\ncountry_code\r\n17831\r\n0.01\r\n2\r\n2\r\n0\r\n17\r\n0\r\ngeo_coords\r\n0\r\n1.00\r\n5\r\n24\r\n0\r\n6\r\n0\r\ncoords_coords\r\n0\r\n1.00\r\n5\r\n24\r\n0\r\n6\r\n0\r\nbbox_coords\r\n0\r\n1.00\r\n23\r\n141\r\n0\r\n93\r\n0\r\nstatus_url\r\n0\r\n1.00\r\n50\r\n62\r\n0\r\n17943\r\n0\r\nname\r\n2\r\n1.00\r\n1\r\n348\r\n0\r\n13270\r\n0\r\nlocation\r\n5727\r\n0.68\r\n1\r\n226\r\n0\r\n4514\r\n0\r\ndescription\r\n2449\r\n0.86\r\n1\r\n1209\r\n0\r\n11324\r\n0\r\nurl\r\n11470\r\n0.36\r\n20\r\n32\r\n0\r\n4366\r\n0\r\nprofile_url\r\n11470\r\n0.36\r\n20\r\n32\r\n0\r\n4366\r\n0\r\nprofile_expanded_url\r\n11473\r\n0.36\r\n12\r\n100\r\n0\r\n4301\r\n0\r\nprofile_banner_url\r\n4458\r\n0.75\r\n54\r\n68\r\n0\r\n10221\r\n0\r\nprofile_background_url\r\n7438\r\n0.59\r\n48\r\n49\r\n0\r\n20\r\n0\r\nprofile_image_url\r\n0\r\n1.00\r\n58\r\n140\r\n0\r\n12872\r\n0\r\nVariable type: logical\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmean\r\ncount\r\nis_quote\r\n0\r\n1.00\r\n0.03\r\nFAL: 17408, TRU: 535\r\nis_retweet\r\n0\r\n1.00\r\n0.65\r\nTRU: 11701, FAL: 6242\r\nquote_count\r\n17943\r\n0.00\r\nNaN\r\n:\r\nreply_count\r\n17943\r\n0.00\r\nNaN\r\n:\r\next_media_type\r\n17943\r\n0.00\r\nNaN\r\n:\r\nquoted_verified\r\n17408\r\n0.03\r\n0.43\r\nFAL: 305, TRU: 230\r\nretweet_verified\r\n6242\r\n0.65\r\n0.47\r\nFAL: 6174, TRU: 5527\r\nprotected\r\n0\r\n1.00\r\n0.00\r\nFAL: 17943\r\nverified\r\n0\r\n1.00\r\n0.04\r\nFAL: 17161, TRU: 782\r\naccount_lang\r\n17943\r\n0.00\r\nNaN\r\n:\r\nVariable type: numeric\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmean\r\nsd\r\np0\r\np25\r\np50\r\np75\r\np100\r\nhist\r\ndisplay_text_width\r\n0\r\n1.00\r\n160.19\r\n51.33\r\n11\r\n140.0\r\n140\r\n150.0\r\n310\r\n▁▁▇▁▁\r\nfavorite_count\r\n0\r\n1.00\r\n1.76\r\n36.34\r\n0\r\n0.0\r\n0\r\n0.0\r\n2560\r\n▇▁▁▁▁\r\nretweet_count\r\n0\r\n1.00\r\n267.42\r\n635.04\r\n0\r\n1.0\r\n4\r\n142.0\r\n7669\r\n▇▁▁▁▁\r\nquoted_favorite_count\r\n17408\r\n0.03\r\n4300.59\r\n17793.73\r\n0\r\n16.5\r\n172\r\n1822.0\r\n284520\r\n▇▁▁▁▁\r\nquoted_retweet_count\r\n17408\r\n0.03\r\n1072.92\r\n3612.48\r\n0\r\n5.0\r\n53\r\n550.5\r\n41710\r\n▇▁▁▁▁\r\nquoted_followers_count\r\n17408\r\n0.03\r\n1265361.30\r\n6196086.27\r\n1\r\n3048.0\r\n25482\r\n178246.0\r\n79078337\r\n▇▁▁▁▁\r\nquoted_friends_count\r\n17408\r\n0.03\r\n3150.25\r\n8832.52\r\n0\r\n442.5\r\n1057\r\n2816.0\r\n111186\r\n▇▁▁▁▁\r\nquoted_statuses_count\r\n17408\r\n0.03\r\n77173.84\r\n163889.98\r\n39\r\n5182.0\r\n19047\r\n60848.5\r\n1244017\r\n▇▁▁▁▁\r\nretweet_favorite_count\r\n6242\r\n0.65\r\n847.39\r\n1957.07\r\n0\r\n9.0\r\n131\r\n1710.0\r\n32501\r\n▇▁▁▁▁\r\nretweet_retweet_count\r\n6242\r\n0.65\r\n409.17\r\n748.52\r\n0\r\n4.0\r\n44\r\n475.0\r\n7669\r\n▇▁▁▁▁\r\nretweet_followers_count\r\n6242\r\n0.65\r\n167132.98\r\n812246.84\r\n0\r\n1868.0\r\n4743\r\n36713.0\r\n16927467\r\n▇▁▁▁▁\r\nretweet_friends_count\r\n6242\r\n0.65\r\n2446.33\r\n7702.08\r\n0\r\n535.0\r\n832\r\n2419.0\r\n280553\r\n▇▁▁▁▁\r\nretweet_statuses_count\r\n6242\r\n0.65\r\n58646.57\r\n126852.08\r\n2\r\n2179.0\r\n10799\r\n50797.0\r\n3221526\r\n▇▁▁▁▁\r\nfollowers_count\r\n0\r\n1.00\r\n19568.67\r\n322503.18\r\n0\r\n109.0\r\n568\r\n2006.0\r\n16927467\r\n▇▁▁▁▁\r\nfriends_count\r\n0\r\n1.00\r\n1772.59\r\n7513.42\r\n0\r\n139.0\r\n579\r\n1728.5\r\n568088\r\n▇▁▁▁▁\r\nlisted_count\r\n0\r\n1.00\r\n99.29\r\n767.37\r\n0\r\n0.0\r\n4\r\n28.0\r\n37942\r\n▇▁▁▁▁\r\nstatuses_count\r\n0\r\n1.00\r\n64414.02\r\n144797.32\r\n1\r\n3259.0\r\n15189\r\n75375.0\r\n3221525\r\n▇▁▁▁▁\r\nfavourites_count\r\n0\r\n1.00\r\n42616.78\r\n89549.84\r\n0\r\n860.0\r\n8151\r\n45395.5\r\n1389145\r\n▇▁▁▁▁\r\nVariable type: POSIXct\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmin\r\nmax\r\nmedian\r\nn_unique\r\ncreated_at\r\n0\r\n1.00\r\n2022-03-24 06:44:16\r\n2022-03-25 20:57:13\r\n2022-03-25 03:32:37\r\n16695\r\nquoted_created_at\r\n17408\r\n0.03\r\n2017-08-08 15:42:11\r\n2022-03-25 20:33:57\r\n2022-03-24 16:05:47\r\n501\r\nretweet_created_at\r\n6242\r\n0.65\r\n2020-03-13 16:08:50\r\n2022-03-25 20:52:46\r\n2022-03-24 17:30:00\r\n2712\r\naccount_created_at\r\n0\r\n1.00\r\n2006-12-12 21:09:52\r\n2022-03-25 18:38:05\r\n2015-06-23 21:17:20\r\n13493\r\n\r\ntweets before cleaning\r\n\r\n\r\ntwitter_data$text[1:10]\r\n\r\n\r\n [1] \"@uk_sf_writer @BBC For the past 2 years @BBCNews have consistently framed #COVID news in the most negative way possible. I have been the following the figures and can inform anyone who is interested that the recent rise in infections has peaked and will go down rapidly next week.\"                                   \r\n [2] \"2022 has brought new #Covid narrative - \\r\\n1 #Virus getting weaker \\r\\n2 #Vaccine situation fine as is\\r\\n3 We don't need #publichealth measures any more\\r\\n\\r\\nReally?\\r\\nSome of my colleagues from academia, @WHO &amp; @ApiJectCorp worked to put together this <U+0001F9F5> \\r\\n#50CovidBeliefs\"                     \r\n [3] \"Today, the Government of <U+0001F1E8><U+0001F1E6> is investing in the health care system to help our provincial and territorial partners expedite the processing of delayed surgeries. This is an important 1st step in repairing #COVID damages and protecting the accessibility of healthcare in <U+0001F1E8><U+0001F1E6>\"\r\n [4] \"Take precautions. A new variant is coming. #COVID https://t.co/ggVR8O2Ty9\"                                                                                                                                                                                                                                                  \r\n [5] \"2022 has brought new #Covid narrative - \\r\\n1 #Virus getting weaker \\r\\n2 #Vaccine situation fine as is\\r\\n3 We don't need #publichealth measures any more\\r\\n\\r\\nReally?\\r\\nSome of my colleagues from academia, @WHO &amp; @ApiJectCorp worked to put together this <U+0001F9F5> \\r\\n#50CovidBeliefs\"                     \r\n [6] \"Daily Covid Deaths per 1K Population by County For GA   2022-03-22:  Latest Covid Insights by Our Analytics Team using USAFacts #datavisualization #datascience #analytics #healthtech #data #covid19 #publichealth #covid #globalhealth #RStats https://t.co/vbeS4sSt2j\"                                                   \r\n [7] \"Daily US Covid Deaths by County For CA   2022-03-22:  Latest Covid Insights by Our Analytics Team using USAFacts #datavisualization #datascience #analytics #healthtech #data #covid19 #publichealth #covid #globalhealth #RStats https://t.co/ZAbfTaGCeS\"                                                                  \r\n [8] \"Total Covid Death Distribution by County  For CA   2022-03-22:  Latest Covid Insights by Our Analytics Team using USAFacts #datavisualization #datascience #analytics #healthtech #data #covid19 #publichealth #covid #globalhealth #RStats https://t.co/1da0nYXgD9\"                                                        \r\n [9] \"Daily US Covid Deaths by County For NC   2022-03-22:  Latest Covid Insights by Our Analytics Team using USAFacts #datavisualization #datascience #analytics #healthtech #data #covid19 #publichealth #covid #globalhealth #RStats https://t.co/7uHHCcf1MY\"                                                                  \r\n[10] \"Total Covid Death Distribution by County  For WA   2022-03-22:  Latest Covid Insights by Our Analytics Team using USAFacts #datavisualization #datascience #analytics #healthtech #data #covid19 #publichealth #covid #globalhealth #RStats https://t.co/RsQdTrdsOn\"                                                        \r\n\r\nvaribales that might be useful in the future analysis : created_at,\r\nscreen_name, text, source, is_retweet, retweet_count, hashtags, symbols,\r\nmentions_screen_name, quoted_text, quoted_created_at,\r\nquoted_screen_name, quoted_name, quoted_followers_count,\r\nquoted_location, quoted_description, retweet_text, retweet_created_at,\r\nretweet_screen_name, retweet_name, retweet_followers_count,\r\nretweet_friends_count, retweet_statuses_count, retweet_location,\r\nretweet_description, place_name, place_type, place_full_name, country,\r\ncountry_code, geo_coords, coords_coords, bbox_coords, name, location,\r\ndescription,\r\ncleaning tweets\r\n\r\n\r\nclean <- function (text) {\r\n  str_remove_all(text,\" ?(f|ht)(tp)(s?)(://)(.*)[.|/](.*)\") %>%\r\n                            # Remove mentions\r\n                            str_remove_all(\"@[[:alnum:]_]*\") %>%\r\n                            # Remove hash tags\r\n                            str_remove_all(\"#[[:alnum:]_]+\") %>%\r\n                            # Replace \"&\" character reference with \"and\"\r\n                            str_replace_all(\"&amp;\", \"and\") %>%\r\n                            # Remove punctuation, using a standard character class\r\n                            str_remove_all(\"[[:punct:]]\") %>%\r\n                            # remove digits\r\n                            str_remove_all(\"[[:digit:]]\") %>%\r\n                            # Remove \"RT: \" from beginning of retweets\r\n                            str_remove_all(\"^RT:? \") %>%\r\n                            # Replace any newline characters with a space\r\n                            str_replace_all(\"\\\\\\n|\\\\\\r\", \" \") %>%\r\n                            # remove strings like \"<U+0001F9F5>\"\r\n                            str_remove_all(\"<.*?>\") %>% \r\n                            # Make everything lowercase\r\n                            str_to_lower() %>%\r\n                            # Remove any trailing white space around the text and inside a string\r\n                            str_squish()\r\n}\r\n\r\n\r\n\r\n\r\n\r\ntwitter_data$text <- clean(twitter_data$text)\r\ntwitter_data$retweet_text <- clean(twitter_data$retweet_text)\r\n\r\n\r\n\r\nlooking at the cleaned tweets\r\n\r\n\r\ntwitter_data$text[1:10]\r\n\r\n\r\n [1] \"for the past years have consistently framed news in the most negative way possible i have been the following the figures and can inform anyone who is interested that the recent rise in infections has peaked and will go down rapidly next week\"              \r\n [2] \"has brought new narrative getting weaker situation fine as is we dont need measures any more really some of my colleagues from academia and worked to put together this\"                                                                                        \r\n [3] \"today the government of is investing in the health care system to help our provincial and territorial partners expedite the processing of delayed surgeries this is an important st step in repairing damages and protecting the accessibility of healthcare in\"\r\n [4] \"take precautions a new variant is coming\"                                                                                                                                                                                                                       \r\n [5] \"has brought new narrative getting weaker situation fine as is we dont need measures any more really some of my colleagues from academia and worked to put together this\"                                                                                        \r\n [6] \"daily covid deaths per k population by county for ga latest covid insights by our analytics team using usafacts\"                                                                                                                                                \r\n [7] \"daily us covid deaths by county for ca latest covid insights by our analytics team using usafacts\"                                                                                                                                                              \r\n [8] \"total covid death distribution by county for ca latest covid insights by our analytics team using usafacts\"                                                                                                                                                     \r\n [9] \"daily us covid deaths by county for nc latest covid insights by our analytics team using usafacts\"                                                                                                                                                              \r\n[10] \"total covid death distribution by county for wa latest covid insights by our analytics team using usafacts\"                                                                                                                                                     \r\n\r\nconverting the character vectors, text,\r\nretweet_text to a single corpus\r\n\r\n\r\ntext <- corpus(c(twitter_data$text, twitter_data$retweet_text))\r\ntext <- dfm(tokens(text, remove_punct=TRUE, remove_numbers = TRUE) %>%\r\n             tokens_select(pattern=stopwords(\"en\"),\r\n                            selection=\"remove\"))\r\n\r\n\r\n\r\nfinding the word frequencies in the documents/ tweets\r\n\r\n\r\nword_counts <- as.data.frame(sort(colSums(text),dec=T))\r\ncolnames(word_counts) <- c(\"Frequency\")\r\nword_counts$word <- row.names(word_counts)\r\nword_counts$Rank <- c(1:ncol(text))\r\nhead(word_counts)\r\n\r\n\r\n       Frequency   word Rank\r\ncovid       7927  covid    1\r\npeople      5364 people    2\r\ncases       3632  cases    3\r\nperson      3398 person    4\r\nnew         3368    new    5\r\nrules       3346  rules    6\r\n\r\ncounting the words that have frequency leass than 50\r\n\r\n\r\nsum(word_counts$Frequency < 50)\r\n\r\n\r\n[1] 12368\r\n\r\nlooking at how word frequencies are distributed\r\n\r\n\r\nggplot(word_counts, mapping = aes(x = Rank, y = Frequency)) + \r\n  geom_point() +\r\n  labs(title = \"Zipf's Law\", x = \"Rank\", y = \"Frequency\") + \r\n  theme_bw()\r\n\r\n\r\n\r\n\r\nHaving seen what we are working with here, we might start to think\r\nthat our matrix still contains too many uninformative or very rare\r\nterms. We can trim our DFM in two different ways related to feature\r\nfrequencies using dfm_trim().\r\n\r\n\r\n# let's create a nicer dfm by limiting to words that appear frequently and are in more than 30% of tweets\r\n#text_dfm <- dfm_trim(text, min_termfreq = 50)\r\ntext_dfm <- dfm_trim(text, min_termfreq = .3, docfreq_type = \"prop\")\r\n\r\n# create fcm from dfm\r\ntext_fcm <- fcm(text_dfm)\r\n\r\n# check the dimensions (i.e., the number of rows and the number of columnns)\r\n# of the matrix we created\r\ndim(text_fcm)\r\n\r\n\r\n[1] 13621 13621\r\n\r\nhead(text_fcm, 10)\r\n\r\n\r\nFeature co-occurrence matrix of: 10 by 13,621 features.\r\n              features\r\nfeatures       past years consistently framed news negative way\r\n  past            3   111           15     15   16       16  15\r\n  years           0    58           15     15   20       17  66\r\n  consistently    0     0            0     15   15       15  15\r\n  framed          0     0            0      0   15       15  15\r\n  news            0     0            0      0   13       16  16\r\n  negative        0     0            0      0    0        2  18\r\n  way             0     0            0      0    0        0  13\r\n  possible        0     0            0      0    0        0   0\r\n  following       0     0            0      0    0        0   0\r\n  figures         0     0            0      0    0        0   0\r\n              features\r\nfeatures       possible following figures\r\n  past               15        15      15\r\n  years             411        18      15\r\n  consistently       15        15      15\r\n  framed             15        15      15\r\n  news               15        17      15\r\n  negative           15        20      15\r\n  way                16        15      15\r\n  possible            2        15      15\r\n  following           0         0      15\r\n  figures             0         0       0\r\n[ reached max_nfeat ... 13,611 more features ]\r\n\r\n\r\n\r\n# pull the top features\r\nmyFeatures <- names(topfeatures(text_fcm, 30))\r\n\r\n# retain only those top features as part of our matrix\r\neven_text_fcm <- fcm_select(text_fcm, pattern = myFeatures, selection = \"keep\")\r\n\r\n# check dimensions\r\ndim(even_text_fcm)\r\n\r\n\r\n[1] 30 30\r\n\r\n# compute size weight for vertices in network\r\nsize <- log(colSums(even_text_fcm))\r\n\r\n# create plot\r\ntextplot_network(even_text_fcm, vertex_size = size / max(size) * 3)\r\n\r\n\r\n\r\n\r\nThe function get_sentiments() allows us to get specific sentiment\r\nlexicons with the appropriate measures for each one. This is a function\r\nin tidytext package\r\nThis dataset nrc was published in Saif M. Mohammad and Peter Turney.\r\n(2013), ``Crowdsourcing a Word-Emotion Association Lexicon.’’\r\nComputational Intelligence, 29(3): 436-465.\r\n\r\n\r\nnrc <- get_sentiments(\"nrc\")\r\n\r\n\r\n\r\nthis dataset afinn was published by Finn Årup Nielsen (http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=6010)\r\n\r\n\r\nafinn <- get_sentiments(\"afinn\")\r\n\r\n\r\n\r\nthis dataset bing was published by Bing Liu and collaborators,https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html\r\n\r\n\r\nbing <- get_sentiments(\"bing\")\r\n\r\n\r\n\r\nsentiment analysis\r\nThe nrc lexicon categorizes words in a binary fashion (“yes”/“no”)\r\ninto categories of positive, negative, anger, anticipation, disgust,\r\nfear, joy, sadness, surprise, and trust.\r\n\r\n\r\n(nrc <- word_counts %>%\r\n  inner_join(nrc, by = \"word\") %>% \r\n  group_by(sentiment) %>% \r\n  summarise(frequency = n()))\r\n\r\n\r\n# A tibble: 10 x 2\r\n   sentiment    frequency\r\n   <chr>            <int>\r\n 1 anger              414\r\n 2 anticipation       380\r\n 3 disgust            315\r\n 4 fear               515\r\n 5 joy                302\r\n 6 negative           991\r\n 7 positive           971\r\n 8 sadness            430\r\n 9 surprise           210\r\n10 trust              546\r\n\r\n  ggplot(nrc, aes(x = sentiment, y = frequency/sum(frequency), fill = sentiment))+\r\n  geom_bar(stat='identity')+\r\n  labs(y = \"percentage of emotion\")+\r\n  theme_classic()\r\n\r\n\r\n\r\n\r\nThe bing lexicon categorizes words in a binary fashion into positive\r\nand negative categories.\r\n\r\n\r\nword_counts %>%\r\n  inner_join(bing, by = \"word\") %>% \r\n  group_by(sentiment) %>% \r\n  summarise(frequency = n()) \r\n\r\n\r\n# A tibble: 2 x 2\r\n  sentiment frequency\r\n  <chr>         <int>\r\n1 negative       1071\r\n2 positive        613\r\n\r\nThe AFINN lexicon assigns words with a score that runs between -5 and\r\n5, with negative scores indicating negative sentiment and positive\r\nscores indicating positive sentiment.\r\n\r\n\r\nword_counts %>%\r\n  inner_join(afinn, by = \"word\") %>% \r\n  group_by(value) %>% \r\n  summarise(frequency = n()) %>% \r\n  arrange(desc(frequency))\r\n\r\n\r\n# A tibble: 10 x 2\r\n   value frequency\r\n   <dbl>     <int>\r\n 1    -2       372\r\n 2     2       236\r\n 3    -1       159\r\n 4     1       148\r\n 5    -3       115\r\n 6     3        71\r\n 7    -4        20\r\n 8     4        19\r\n 9    -5         4\r\n10     5         3\r\n\r\nmost common positive and negative words\r\n\r\n\r\nhead(bing_word_counts <- word_counts %>%\r\n  inner_join(get_sentiments(\"bing\")) %>%\r\n   select(word, Frequency, sentiment), 30)\r\n\r\n\r\n        word Frequency sentiment\r\n1      limit      3277  negative\r\n2      death      1702  negative\r\n3     worked      1557  positive\r\n4       fine      1555  positive\r\n5     weaker      1542  negative\r\n6   positive      1042  positive\r\n7       like       996  positive\r\n8       died       797  negative\r\n9       safe       726  positive\r\n10  symptoms       703  negative\r\n11     virus       699  negative\r\n12  recovery       661  positive\r\n13   protect       635  positive\r\n14   revival       620  positive\r\n15     rapid       613  positive\r\n16      work       534  positive\r\n17      best       500  positive\r\n18 available       419  positive\r\n19     break       408  negative\r\n20     bravo       400  positive\r\n21     right       384  positive\r\n22      dead       382  negative\r\n23       top       381  positive\r\n24       mar       346  negative\r\n25      free       340  positive\r\n26      sick       340  negative\r\n27      risk       337  negative\r\n28      good       327  positive\r\n29      well       289  positive\r\n30    better       265  positive\r\n\r\n\r\n\r\nhead(nrc_word_counts <- word_counts %>%\r\n  inner_join(get_sentiments(\"nrc\")) %>%\r\n   filter(sentiment %in% c(\"positve\", \"negative\")) %>% \r\n   select(word, Frequency, sentiment), 30)\r\n\r\n\r\n          word Frequency sentiment\r\n1        death      1702  negative\r\n2     pandemic      1287  negative\r\n3   government       710  negative\r\n4        virus       699  negative\r\n5      disease       670  negative\r\n6          hit       560  negative\r\n7         wear       478  negative\r\n8         case       421  negative\r\n9         shot       408  negative\r\n10         mar       346  negative\r\n11        sick       340  negative\r\n12        risk       337  negative\r\n13         war       335  negative\r\n14   infection       253  negative\r\n15       spent       246  negative\r\n16      crisis       225  negative\r\n17       sadly       224  negative\r\n18       fraud       221  negative\r\n19       fight       216  negative\r\n20    negative       215  negative\r\n21     refused       214  negative\r\n22 enforcement       201  negative\r\n23      demand       195  negative\r\n24        hate       193  negative\r\n25       worse       191  negative\r\n26   suffering       190  negative\r\n27   abandoned       187  negative\r\n28     illness       177  negative\r\n29         bad       171  negative\r\n30         flu       170  negative\r\n\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-03-28T16:55:55-04:00",
    "input_file": "fourth_post.knit.md"
  },
  {
    "path": "posts/third_post/",
    "title": "Initial Analysis",
    "description": "Initial Understanding of the Data.",
    "author": [
      {
        "name": "Sathvik Thogaru",
        "url": "https://thogarusathvik.github.io/Text-as-Data-Project/"
      }
    ],
    "date": "2022-02-22",
    "categories": [],
    "contents": "\r\n\r\nbody {\r\ntext-align: justify}\r\nThis is just an initial glimpse at the how will the data look like\r\nonce I download using the API. I have created a word cloud using the\r\ninitially collected 200 observations of data( data is not cleaned\r\nproperly) from the Twitter using the twitter API’s and the hashtag used\r\nis #COVID19\r\n\r\n\r\nlibrary(quanteda)\r\nlibrary(tidyr)\r\nlibrary(quanteda.textplots)\r\n\r\n\r\n\r\nSince the API Keys and Tokens used needs to secured and cannot be\r\nshared I didn’t include my data scrapping code in this post. Below is\r\nthe syntax that I used to get the tweets.\r\nlibrary(rtweet)\r\nmytoken <- create_token(\r\napp = \"scrapetext\",\r\nconsumer_key = \"#############\",\r\nconsumer_secret = \"##############\",\r\naccess_token = \"#############\",\r\naccess_secret = \"#############\")\r\ndata <- search_tweets(\"#Covid\", include_retweets = TRUE,\r\nn = 1000, retryonratelimit = TRUE,  token = mytoken, lang = 'en')\r\nsave(data, file = 'twitter_data.rda')\r\nusing the above R code I extracted the 1000 tweets which\r\nare in English. The retweets are also\r\nincluded\r\npersonal twitter dev account, using twitter API extracted the 200\r\nmixed type of tweets which are in english. The retweets are also\r\nincluded\r\n\r\n\r\ndata_ <- load(file = '../../twitter_data.rda')\r\n\r\n\r\n\r\nrange of data available\r\n\r\n\r\nrange(data$created_at)\r\n\r\n\r\n[1] \"2022-02-22 00:39:01 UTC\" \"2022-02-23 07:52:00 UTC\"\r\n\r\nhastags that are included in the tweets\r\n\r\n\r\nhashtags <- list()\r\nfor (i in data$hashtags){hashtags <- c(hashtags, i)}\r\n\r\n\r\n\r\nbuilding wordcloud\r\n\r\n\r\nhashtags_c = unlist(hashtags)\r\ntextplot_wordcloud(dfm(corpus(hashtags_c)))\r\n\r\n\r\n\r\n\r\nconverting the character vectors, text,\r\nretweet_text to a single corpus\r\n\r\n\r\ntext <- corpus(c(data$text, data$retweet_text))\r\ntext <- tokens(text, remove_punct=TRUE, remove_numbers = TRUE) %>%\r\n             tokens_select(pattern=stopwords(\"en\"),\r\n                            selection=\"remove\")\r\n\r\n\r\n\r\nremoving hashtags removing links\r\n\r\n\r\ntext <- sub(\"(?:\\\\s*#\\\\w+)+\\\\s*$\", \"\", text)\r\ntext <- gsub(\"http.+\", \"\", text) \r\n\r\n\r\n\r\nvisual representation of the tweet and retweet text data that is\r\navailable in the collected data\r\n\r\n\r\ntextplot_wordcloud(dfm(text))\r\n\r\n\r\n\r\n\r\nlooks like there are a lot of region based language text used in the\r\ntweets, this may be the reasin the words are not really meaningful.\r\n\r\n\r\n\r\n",
    "preview": "posts/third_post/third_post_files/figure-html5/unnamed-chunk-5-1.png",
    "last_modified": "2022-03-16T01:51:12-04:00",
    "input_file": "third_post.knit.md"
  },
  {
    "path": "posts/second_post/",
    "title": "Second Post: Data Collection",
    "description": "Data collection and Characteristics of Data Source",
    "author": [
      {
        "name": "Sathvik Thogaru",
        "url": "https://thogarusathvik.github.io/Text-as-Data-Project/"
      }
    ],
    "date": "2022-02-21",
    "categories": [],
    "contents": "\r\n\r\nbody {\r\ntext-align: justify}\r\nFor my Research Question which I have stated in my First post, I will\r\ncollect the Tweets of all the possible Covid19\r\nData that I can get. The Data will be collected using the\r\nTwitter API. The Twitter API helps to read and\r\nwrite Twitter data. It is also helpful to\r\ncompose tweets, read profiles, and\r\naccess followers' data and a high volume of tweets on\r\nparticular subjects in specific locations. API stands for\r\nApplication Programming Interface.\r\nTo get access to Twitter API, I will first sign up for\r\nTwitter Developer Account,then get access to my\r\nAPI keys and Tokens. After getting the\r\nrequired Keys and Tokens I make my initial request by using the\r\nsearch_tweets() fuction in package rtweet and\r\nstore the data in a file which I will be using for my research.\r\nThe initial glimpse of the data and its content will be availabe in\r\nthe next post.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-03-16T01:53:32-04:00",
    "input_file": "second_post.knit.md"
  },
  {
    "path": "posts/welcome/",
    "title": "First Post: Introduction",
    "description": "Welcome to my blog Text as data, please click to check initial draft. \n Kindly look for future posts for furthur information. New posts every week :-)...",
    "author": [
      {
        "name": "Sathvik Thogaru",
        "url": "https://thogarusathvik.github.io/Text-as-Data-Project/"
      }
    ],
    "date": "2022-02-18",
    "categories": [],
    "contents": "\r\n\r\nbody {\r\ntext-align: justify}\r\nTwitter, Facebook, Reddit and other social media platforms represent\r\na large and largely untapped resource for social data and evidence.The\r\ndata in these platforms is vast, contains variety of topics that people\r\ndiscuss on the daily basis regardless of geographical location. This\r\ndata that is generated on the daily basis provides a key/main source of\r\ninformation for anyone who wants to study science, communication, people\r\nopinions and situations happening across the world at the same time.\r\nThese platforms provide data through number of applicatyion\r\nProgramming Interfaces(API). Although it may be diificult or sometimes\r\nnot possible to obtain data from these platforms, they help in\r\nqualitative and quantitative research.\r\nCovid-19 outbreak was nothing less than the biggest nightmare the\r\nhumanity has gone through that has changed the lives of millions\r\nupsidedown in no time and almost everyone have suffered. Even with the\r\nresearch advancements we see and develop every single day, yet we still\r\nhaven’t figured out completely to eliminate the virus. So we adapted\r\nourselves to live with it but that didn’t happen overnight. In my study\r\nI am trying to collect the voices of people across all borders what they\r\nshared on social media platforms like Twitter, Reddit etc., and\r\ndifferentiate the data according to the phases of the outbreak while\r\nlisting the most common problems that masses faced at each stage and\r\nestablish the relation on how the trend shifting happened overtime and\r\nwho were most impacted by it.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-03-16T01:35:23-04:00",
    "input_file": "welcome.knit.md"
  }
]

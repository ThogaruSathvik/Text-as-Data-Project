[
  {
    "path": "posts/seventh_post/",
    "title": "STM",
    "description": "STM",
    "author": [
      {
        "name": "Sathvik Thogaru",
        "url": "https://thogarusathvik.github.io/Text-as-Data-Project/"
      }
    ],
    "date": "2022-05-05",
    "categories": [],
    "contents": "\r\n\r\nbody {\r\ntext-align: justify}\r\n\r\n\r\nlibrary(topicmodels)\r\nlibrary(dplyr)\r\nlibrary(tm)\r\nlibrary(quanteda)\r\nlibrary(tidyr)\r\nlibrary(readr)\r\nlibrary(stringr)\r\nlibrary(tidytext)\r\nlibrary(stopwords)\r\nlibrary(stm)\r\n\r\n\r\n\r\n\r\n\r\ndf1 <- read_csv(\"../../data/combined_data.csv\") %>% select(-1)\r\ndf1$word_count <- str_count(df1$text, \"\\\\w+\")\r\ndf1 <-  df1[!df1$word_count<6,] ## 40227 rows\r\n\r\n\r\n\r\n\r\n\r\nstop_words <- stopwords(source = \"stopwords-iso\")\r\nstop_words <- as.data.frame(stop_words)\r\n\r\n\r\n\r\n\r\n\r\ndf2 <- df1 %>%\r\n  unnest_tokens(text, text)\r\ndf2 <- df2 %>% \r\n  anti_join(stop_words, by= c(\"text\" = \"stop_words\"))\r\n\r\ndf2 <- df2 %>% \r\n  group_by(tweet_id) %>%\r\n  summarise(text = paste0(text, collapse = ' '))\r\n\r\ndf2$word_count <- str_count(df2$text, \"\\\\w+\")\r\ndf2 <-  df2[!df2$word_count<6,]\r\n\r\n\r\n\r\n\r\n\r\ntweets1 <- df2$text\r\ntweets1 <- unique(tweets1)\r\ncorpus1 <- corpus(tweets1)\r\ncorpus1 <- corpus_trim(corpus1, min_ntoken = 6)\r\n\r\n\r\n\r\n\r\n\r\ndfm <- dfm(corpus1)\r\n# keep only words occurring >= 10 times and in >= 10 documents\r\ndfm <- dfm_trim(dfm, min_docfreq = 10, min_termfreq = 10)\r\n\r\n\r\n\r\n\r\n\r\nstm_dfm <- convert(dfm, to = \"stm\")\r\n\r\n\r\n\r\n\r\n\r\nstm_out <- prepDocuments(stm_dfm$documents, \r\n                               stm_dfm$vocab, \r\n                               stm_dfm$meta, \r\n                     lower.thresh = 10)\r\n\r\n\r\nRemoving 1612 of 28830 terms (16120 of 5347321 tokens) due to frequency \r\nRemoving 1 Documents with No Words \r\nYour corpus now has 485952 documents, 27218 terms and 5331201 tokens.\r\n\r\n\r\n\r\nstmResult <- stm(stm_out$documents, stm_out$vocab, K = 10,\r\n                          max.em.its = 75, data = stm_out$meta, init.type = \"Spectral\", seed = 1234)\r\n\r\n\r\nBeginning Spectral Initialization \r\n     Calculating the gram matrix...\r\n     Using only 10000 most frequent terms during initialization...\r\n     Finding anchor words...\r\n    ..........\r\n     Recovering initialization...\r\n    ....................................................................................................\r\nInitialization complete.\r\n....................................................................................................\r\nCompleted E-Step (134 seconds). \r\nCompleted M-Step. \r\nCompleting Iteration 1 (approx. per word bound = -8.770) \r\n....................................................................................................\r\nCompleted E-Step (154 seconds). \r\nCompleted M-Step. \r\nCompleting Iteration 2 (approx. per word bound = -8.361, relative change = 4.661e-02) \r\n....................................................................................................\r\nCompleted E-Step (135 seconds). \r\nCompleted M-Step. \r\nCompleting Iteration 3 (approx. per word bound = -8.212, relative change = 1.782e-02) \r\n....................................................................................................\r\nCompleted E-Step (150 seconds). \r\nCompleted M-Step. \r\nCompleting Iteration 4 (approx. per word bound = -8.155, relative change = 7.009e-03) \r\n....................................................................................................\r\nCompleted E-Step (155 seconds). \r\nCompleted M-Step. \r\nCompleting Iteration 5 (approx. per word bound = -8.128, relative change = 3.235e-03) \r\nTopic 1: day, data, team, county, days \r\n Topic 2: booster, vaccine, hospital, dose, jan \r\n Topic 3: pandemic, health, ukraine, crisis, global \r\n Topic 4: stay, jobs, rt, business, stayhome \r\n Topic 5: coronavirus, omicron, deaths, india, news \r\n Topic 6: testing, johnson, boris, restrictions, students \r\n Topic 7: stay, food, safe, health, healthy \r\n Topic 8: people, vaccine, vaccines, vaccinated, vaccination \r\n Topic 9: vaccine, canada, freedom, mandates, convoy \r\n Topic 10: pandemic, health, people, vaccine, time \r\n....................................................................................................\r\nCompleted E-Step (167 seconds). \r\nCompleted M-Step. \r\nCompleting Iteration 6 (approx. per word bound = -8.115, relative change = 1.671e-03) \r\n....................................................................................................\r\nCompleted E-Step (152 seconds). \r\nCompleted M-Step. \r\nCompleting Iteration 7 (approx. per word bound = -8.107, relative change = 9.089e-04) \r\n....................................................................................................\r\nCompleted E-Step (131 seconds). \r\nCompleted M-Step. \r\nCompleting Iteration 8 (approx. per word bound = -8.104, relative change = 4.572e-04) \r\n....................................................................................................\r\nCompleted E-Step (102 seconds). \r\nCompleted M-Step. \r\nCompleting Iteration 9 (approx. per word bound = -8.102, relative change = 1.658e-04) \r\n....................................................................................................\r\nCompleted E-Step (153 seconds). \r\nCompleted M-Step. \r\nModel Converged \r\n\r\n# save.image(file = \"seventh_post.RData\")\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nplot(stmResult, type = \"summary\")\r\n\r\n\r\n\r\ncloud(stmResult, topic = 10, scale = c(2, 0.25))\r\n\r\n\r\n\r\nmod.out.corr <- topicCorr(stmResult)\r\nplot(mod.out.corr)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/seventh_post/seventh_post_files/figure-html5/plotting-1.png",
    "last_modified": "2022-05-14T13:22:06-04:00",
    "input_file": "seventh_post.knit.md"
  },
  {
    "path": "posts/sixth_post/",
    "title": "Sixth post",
    "description": "LDA",
    "author": [
      {
        "name": "Sathvik Thogaru",
        "url": "https://thogarusathvik.github.io/Text-as-Data-Project/"
      }
    ],
    "date": "2022-04-26",
    "categories": [],
    "contents": "\r\n\r\nbody {\r\ntext-align: justify}\r\n\r\n\r\nlibrary(ggplot2)\r\nlibrary(topicmodels)\r\nlibrary(dplyr)\r\nlibrary(tm)\r\nlibrary(quanteda)\r\nlibrary(tidyr)\r\nlibrary(readr)\r\nlibrary(stringr)\r\nlibrary(tidytext)\r\nlibrary(stopwords)\r\nlibrary(textmineR)\r\nlibrary(stm)\r\n# load(\"../../data/sixth_post.RData\")\r\n\r\n\r\n\r\nreading data\r\nAfter cleaning, the tweets that have length less than six words are\r\nremoved from the analysis. Stemming is not preferred here as the meaning\r\nof the word is important for analysis\r\n\r\n\r\ndf1 <- read_csv(\"../../data/combined_data.csv\") %>% select(-1)\r\ndf1$word_count <- str_count(df1$text, \"\\\\w+\")\r\ndf1 <-  df1[!df1$word_count<6,] ## 40227 rows\r\n\r\n\r\n\r\nremoving the stopwords\r\n\r\n\r\nstop_words <- stopwords(source = \"stopwords-iso\")\r\nstop_words <- as.data.frame(stop_words)\r\n\r\n\r\n\r\nremoving\r\n\r\n\r\ndf2 <- df1 %>%\r\n  unnest_tokens(text, text)\r\ndf2 <- df2 %>% \r\n  anti_join(stop_words, by= c(\"text\" = \"stop_words\"))\r\n\r\ndf2 <- df2 %>% \r\n  group_by(tweet_id) %>%\r\n  summarise(text = paste0(text, collapse = ' '))\r\n\r\ndf2$word_count <- str_count(df2$text, \"\\\\w+\")\r\ndf2 <-  df2[!df2$word_count<6,]\r\n\r\n\r\n\r\ncorpus is trimmed with document having atleast 6 tokens\r\n\r\n\r\ntweets1 <- df2$text\r\ntweets1 <- unique(tweets1)\r\ncorpus1 <- corpus(tweets1)\r\ncorpus1 <- corpus_trim(corpus1, min_ntoken = 6)\r\n\r\n\r\n\r\nLDA is run over 2, 5 nd 10 topics\r\n\r\n\r\ndfm <- dfm(corpus1)\r\n# keep only words occurring >= 10 times and in >= 10 documents\r\ndfm <- dfm_trim(dfm, min_docfreq = 10, min_termfreq = 10)\r\ndtm <- convert(dfm, to = \"topicmodels\")\r\n# dtms1 <- removeSparseTerms(dtm1, 0.99)\r\nlda_10 <-  LDA(dtm, k = 10, method = 'Gibbs', control = list(seed = 1234))\r\n# save.image(file = \" sixth_post.RData\")\r\nlda_5 <-  LDA(dtm, k = 5, method = 'Gibbs', control = list(seed = 1234))\r\n# save.image(file = \" sixth_post.RData\")\r\nlda_2 <-  LDA(dtm, k = 2, method = 'Gibbs', control = list(seed = 1234))\r\n# save.image(file = \" sixth_post.RData\")\r\n\r\n\r\n\r\n\r\n\r\n#Top 10 terms or words under each topic\r\n(top10terms_2 <- as.data.frame(as.matrix(terms(lda_2,10))))\r\n\r\n\r\n      Topic 1     Topic 2\r\n1     vaccine coronavirus\r\n2      people    pandemic\r\n3    vaccines     omicron\r\n4        mask      health\r\n5  vaccinated      deaths\r\n6        time        news\r\n7    children         day\r\n8       masks       india\r\n9        care vaccination\r\n10 government    positive\r\n\r\n(top10terms_5 <- as.data.frame(as.matrix(terms(lda_5,10))))\r\n\r\n\r\n      Topic 1     Topic 2 Topic 3   Topic 4     Topic 5\r\n1      people     vaccine    mask  pandemic coronavirus\r\n2      canada      health    time    health     omicron\r\n3     freedom    vaccines   masks      data      deaths\r\n4    mandates vaccination  public      read        news\r\n5  government       virus    stay   support       india\r\n6     johnson        care    safe    global    positive\r\n7      convoy  vaccinated  school    report      corona\r\n8     ukraine    children    kids community     variant\r\n9        died     booster schools countries         day\r\n10      lives          dr     day      team       tests\r\n\r\n(top10terms_10 <-  as.data.frame(as.matrix(terms(lda_10,10))))\r\n\r\n\r\n   Topic 1   Topic 2   Topic 3  Topic 4     Topic 5      Topic 6\r\n1   people      risk    people   health     vaccine         mask\r\n2  johnson  patients  positive pandemic    vaccines        masks\r\n3  ukraine        dr   testing  support vaccination   government\r\n4    biden     study      time   global  vaccinated restrictions\r\n5   russia infection     tests    learn    children       school\r\n6    lives      sars    tested response     booster      schools\r\n7    boris     virus      days   impact    mandates       spread\r\n8     anti   disease  symptoms    watch        kids         wear\r\n9    trump   science    family    video     mandate       public\r\n10     war treatment australia   public        dose     continue\r\n      Topic 7     Topic 8  Topic 9  Topic 10\r\n1        stay coronavirus   deaths      data\r\n2        care     omicron      day     china\r\n3        safe        news   canada    report\r\n4  healthcare       india  freedom countries\r\n5     medical      corona    death      team\r\n6    stayhome     variant reported       usa\r\n7     workers       virus     week    travel\r\n8        life        live    daily    county\r\n9        jobs     updates     rate      read\r\n10   business      follow   convoy  national\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n# apply auto tidy using tidy and use beta as per-topic-per-word probabilities\r\n\r\ntopic_2 <- tidy(lda_2,matrix = \"beta\")\r\n\r\n# choose 10 words with highest beta from each topic\r\n\r\ntop_terms_2 <- topic_2 %>%\r\n  group_by(topic) %>%\r\n  top_n(10,beta) %>%\r\n  ungroup() %>%\r\n  arrange(topic,-beta)\r\n\r\n# plot the topic and words for easy interpretation\r\n\r\nplot_topic_2 <- top_terms_2 %>%\r\n  mutate(term = reorder_within(term, beta, topic)) %>%\r\n  ggplot(aes(term, beta, fill = factor(topic))) +\r\n  geom_col(show.legend = FALSE) +\r\n  facet_wrap(~ topic, scales = \"free\") +\r\n  coord_flip() +\r\n  scale_x_reordered()\r\n\r\nplot_topic_2\r\n\r\n\r\n\r\n\r\n\r\n\r\ntopic_5 <- tidy(lda_5,matrix = \"beta\")\r\n  top_terms_5 <- topic_5 %>%\r\n  group_by(topic) %>%\r\n    top_n(10,beta) %>%\r\n   ungroup() %>%\r\n   arrange(topic,-beta)\r\n\r\nplot_topic_5 <- top_terms_5 %>%\r\n   mutate(term = reorder_within(term, beta, topic)) %>%\r\n   ggplot(aes(term, beta, fill = factor(topic))) +\r\n   geom_col(show.legend = FALSE) +\r\n   facet_wrap(~ topic, scales = \"free\") +\r\n   coord_flip() +\r\n   scale_x_reordered()\r\n\r\nplot_topic_5\r\n\r\n\r\n\r\n\r\n\r\n\r\ntopic_10 <- tidy(lda_10,matrix = \"beta\")\r\n\r\ntop_terms_10 <- topic_10 %>%\r\n  group_by(topic) %>%\r\n  top_n(10,beta) %>%\r\n  ungroup() %>%\r\n  arrange(topic,-beta)\r\n\r\nplot_topic_10 <- top_terms_10 %>%\r\n  mutate(term = reorder_within(term, beta, topic)) %>%\r\n  ggplot(aes(term, beta, fill = factor(topic))) +\r\n  geom_col(show.legend = FALSE) +\r\n  facet_wrap(~ topic, scales = \"free\") +\r\n  coord_flip() +\r\n  scale_x_reordered()\r\n\r\nplot_topic_10\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nlibrary(wordcloud)\r\nwordcloud(names(top_words), top_words)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\ntopdoc = names(topic.docs)[1]\r\ntopdoc_corp = corpus1[docnames(corpus1) == topdoc]\r\ntexts(topdoc_corp)\r\n\r\n\r\n                                                                                                                                                                                                                  text268712 \r\n\"mask distribution dss volunteers pious teachings saint dr gurmeet singh ji insan khanna ludhiana punjab dera sacha sauda welfare saint ram rahim ji saint dr msg saint dr gurmeet ram rahim singh ji mask mask india masks\" \r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/sixth_post/sixth_post_files/figure-html5/LDA_2_topics-1.png",
    "last_modified": "2022-05-14T12:41:41-04:00",
    "input_file": "sixth_post.knit.md"
  },
  {
    "path": "posts/fifth_post/",
    "title": "fifth_post",
    "description": "Blog Post 5",
    "author": [
      {
        "name": "Sathvik Thogaru",
        "url": "https://thogarusathvik.github.io/Text-as-Data-Project/"
      }
    ],
    "date": "2022-04-18",
    "categories": [],
    "contents": "\r\n\r\nbody {\r\ntext-align: justify}\r\nintro\r\nloading required libraries\r\nAll the twitter data in the JSON format. It is later converted to csv\r\nformat and stored in the combined_data.csv file which is loaded here\r\n\r\n\r\nlibrary(quanteda)\r\nlibrary(tidyr)\r\nlibrary(quanteda.textplots)\r\nlibrary(dplyr)\r\nlibrary(readr)\r\nlibrary(stopwords)\r\nlibrary(ggplot2)\r\nlibrary(tidytext)\r\nlibrary(quanteda.sentiment)\r\nlibrary(plotrix)\r\nlibrary(radarchart)\r\n#load(\"../../data/fifth_post.RData\")\r\n\r\n\r\n\r\ndf <- read_csv(\"../../data/combined_data.csv\") %>% select(-1)\r\n\r\n\r\n\r\nNow we just wnt to know where more tweets are from?\r\n\r\n\r\ndf %>% filter(!is.na(user_location)) %>% \r\n  count(user_location, sort = TRUE) %>%\r\n  mutate(user_location = reorder(user_location, n)) %>%\r\n  top_n(10) %>%\r\n  ggplot(aes(x = user_location, y = n, na.rm = TRUE, fill = n)) +\r\n  geom_col() +\r\n  coord_flip() +\r\n      labs(x = \"Location\",\r\n      y = \"Count\",\r\n      title = \"Where more tweets are from - unique locations \") +  theme_classic()+\r\n  theme(legend.position=\"none\")\r\n\r\n\r\n\r\n\r\nLooks like most of the tweets are from India, followed by the USA\r\nsates and England countries.\r\ntokenization and dfm\r\nstop words are removed using the collection ” stopwords-iso”. the\r\nstopwords-iso has 1298 unique stopwords in english language\r\n\r\n\r\ntweets <- df$text\r\nstop_words <- stopwords(source = \"stopwords-iso\")\r\ntweets <- tweets %>% tokens %>% \r\n  tokens_remove(pattern = phrase(stop_words), valuetype = 'fixed')\r\n\r\ndfm_tokens <- dfm(tweets)\r\ntopfeatures(dfm_tokens, 20)\r\n\r\n\r\n     people     vaccine coronavirus     omicron      deaths \r\n     167343      163866      149677      137526      137209 \r\n     health    pandemic    vaccines        data         day \r\n     130645      126603       92790       89259       85621 \r\n       mask        kids vaccination    children  vaccinated \r\n      84397       70541       70511       63841       62737 \r\n  analytics          dr       virus    positive        news \r\n      62421       59214       58719       57913       57142 \r\n\r\nwordcloud of all the tokens\r\n\r\n\r\nword_counts <- as.data.frame(sort(colSums(dfm_tokens),dec=T))\r\ncolnames(word_counts) <- c(\"count\")\r\nword_counts$word <- row.names(word_counts)\r\ntextplot_wordcloud(dfm_tokens)\r\n\r\n\r\n\r\n\r\nLokks like the discussions were around omicron, vaccines, pandemic,\r\nhealthcare and deaths caused by coronavirus virus\r\nsave RData\r\n\r\n\r\n# save.image(file = \"fifth_post.RData\")\r\n\r\n\r\n\r\nfcm\r\n\r\n\r\ntext_dfm <- dfm_trim(dfm_tokens, min_termfreq = 500)\r\n\r\n# create fcm from dfm\r\ntext_fcm <- fcm(text_dfm)\r\n\r\n# check the dimensions (i.e., the number of rows and the number of columnns)\r\n# of the matrix we created\r\ndim(text_fcm)\r\n\r\n\r\n[1] 4961 4961\r\n\r\n\r\n\r\n# pull the top features\r\nmyFeatures <- names(topfeatures(text_fcm, 50))\r\n\r\n# retain only those top features as part of our matrix\r\ntop_text_fcm <- fcm_select(text_fcm, pattern = myFeatures, selection = \"keep\")\r\n\r\n# check dimensions\r\ndim(top_text_fcm)\r\n\r\n\r\n[1] 50 50\r\n\r\n\r\n\r\n# compute size weight for vertices in network\r\nsize <- log(colSums(top_text_fcm))\r\n\r\n# create plot\r\ntextplot_network(top_text_fcm, vertex_size = size / max(size) * 3)\r\n\r\n\r\n\r\n\r\nsentiment analysis bing and\r\nnrc\r\n\r\n\r\ntweets_bing<-word_counts%>% \r\n  # Implement sentiment analysis using the \"bing\" lexicon\r\n  inner_join(get_sentiments(\"bing\")) \r\n\r\nperc<-tweets_bing %>% \r\n  count(sentiment)%>% #count sentiment\r\n  mutate(total=sum(n)) %>% #get sum\r\n  group_by(sentiment) %>% #group by sentiment\r\n  mutate(percent=round(n/total,2)*100) %>% #get the proportion\r\n  ungroup()\r\n\r\nlabel <-c( paste(perc$percent[1],'%',' - ',perc$sentiment[1],sep=''),#create label\r\n     paste(perc$percent[2],'%',' - ',perc$sentiment[2],sep=''))\r\n\r\npie3D(perc$percent,labels=label,labelcex=1.1,explode= 0.1, \r\n      main=\"Worldwide Sentiment\")\r\n\r\n\r\n\r\n\r\nPeople showed more negative sentiments during the third wave in\r\nJanuary and February months\r\n\r\n\r\nword_counts %>%\r\n  # implement sentiment analysis using the \"nrc\" lexicon\r\n  inner_join(get_sentiments(\"nrc\")) %>%\r\n  # remove \"positive/negative\" sentiments\r\n  filter(!sentiment %in% c(\"positive\", \"negative\")) %>%\r\n  #get the frequencies of sentiments\r\n  count(sentiment,sort = T) %>% \r\n  #calculate the proportion\r\n  mutate(percent=100*n/sum(n)) %>%\r\n  select(sentiment, percent) %>%\r\n  #plot the result\r\n  chartJSRadar(showToolTipLabel = TRUE, main = \"NRC Radar\")\r\n\r\n\r\n{\"x\":{\"data\":{\"labels\":[\"fear\",\"anger\",\"trust\",\"sadness\",\"disgust\",\"anticipation\",\"joy\",\"surprise\"],\"datasets\":[{\"label\":\"percent\",\"data\":[17.9798771723507,15.3534561609826,14.765451456945,14.4126486345224,12.6747680648112,10.1920815366523,8.16673200052267,6.45498497321312],\"backgroundColor\":\"rgba(255,0,0,0.2)\",\"borderColor\":\"rgba(255,0,0,0.8)\",\"pointBackgroundColor\":\"rgba(255,0,0,0.8)\",\"pointBorderColor\":\"#fff\",\"pointHoverBackgroundColor\":\"#fff\",\"pointHoverBorderColor\":\"rgba(255,0,0,0.8)\"}]},\"options\":{\"responsive\":true,\"title\":{\"display\":true,\"text\":\"NRC Radar\"},\"scale\":{\"ticks\":{\"min\":0},\"pointLabels\":{\"fontSize\":18}},\"tooltips\":{\"enabled\":true,\"mode\":\"label\"},\"legend\":{\"display\":true}}},\"evals\":[],\"jsHooks\":[]}\r\nPeople display anger, disgust, fear, and sadness toward the pandemic\r\nand related death and there is less joy and anticipation.\r\n\r\n\r\n\r\n",
    "preview": "posts/fifth_post/fifth_post_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2022-05-14T12:13:09-04:00",
    "input_file": "fifth_post.knit.md"
  },
  {
    "path": "posts/fouth_post/",
    "title": "Sentiment Analysis",
    "description": "Sentiment Analysis.",
    "author": [
      {
        "name": "Sathvik Thogaru",
        "url": "https://thogarusathvik.github.io/Text-as-Data-Project/"
      }
    ],
    "date": "2022-03-10",
    "categories": [],
    "contents": "\r\n\r\n\r\npre code {\r\n  white-space: pre-wrap;\r\n}\r\n\r\n\r\n\r\nbody {\r\ntext-align: justify}\r\n\r\n\r\nlibrary(devtools)\r\n#devtools::install_github(\"kbenoit/quanteda.dictionaries\") \r\nlibrary(quanteda.dictionaries)\r\n#devtools::install_github(\"quanteda/quanteda.sentiment\")\r\nlibrary(quanteda.sentiment)\r\n\r\n\r\n\r\n\r\n\r\nlibrary(readr)\r\nlibrary(lubridate)\r\nlibrary(skimr)\r\nlibrary(ggplot2)\r\nlibrary(stringr)\r\nlibrary(dplyr)\r\nlibrary(quanteda)\r\nlibrary(quanteda.textplots)\r\nlibrary(tidytext)\r\n\r\ntwitter_data <- read_csv(\"../../data/combined_data.csv\") %>% select(-1)\r\n\r\n\r\n\r\nsummary of the data\r\n\r\n\r\nskim(twitter_data)\r\n\r\n\r\nTable 1: Data summary\r\nName\r\ntwitter_data\r\nNumber of rows\r\n2140951\r\nNumber of columns\r\n5\r\n_______________________\r\n\r\nColumn type frequency:\r\n\r\ncharacter\r\n3\r\nnumeric\r\n1\r\nPOSIXct\r\n1\r\n________________________\r\n\r\nGroup variables\r\nNone\r\nVariable type: character\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmin\r\nmax\r\nempty\r\nn_unique\r\nwhitespace\r\nuser_name\r\n173\r\n1.00\r\n1\r\n400\r\n0\r\n600604\r\n0\r\ntext\r\n3028\r\n1.00\r\n1\r\n295\r\n0\r\n570851\r\n0\r\nuser_location\r\n722541\r\n0.66\r\n1\r\n692\r\n0\r\n128828\r\n0\r\nVariable type: numeric\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmean\r\nsd\r\np0\r\np25\r\np50\r\np75\r\np100\r\nhist\r\ntweet_id\r\n0\r\n1\r\n1.489498e+18\r\n4.495719e+15\r\n1.483113e+18\r\n1.485359e+18\r\n1.489598e+18\r\n1.493456e+18\r\n1.498086e+18\r\n▇▃▅▅▃\r\nVariable type: POSIXct\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmin\r\nmax\r\nmedian\r\nn_unique\r\ncreated_at\r\n0\r\n1\r\n2022-01-17 16:24:12\r\n2022-02-27 23:59:59\r\n2022-02-04 13:54:15\r\n1479653\r\n\r\ntweets before cleaning\r\n\r\n\r\ntwitter_data$text[1:10]\r\n\r\n\r\n [1] \"the dashboard has been updated on january new cases and deaths in days of\"                                                                                \r\n [2] \"health canada has authorized the first treatment in pill form that can be used at home to help prevent ser\"                                               \r\n [3] \"vaccine year of vaccination my gettr\"                                                                                                                     \r\n [4] \"hey unvaccinated people what your excuse now even sheep are doing it\"                                                                                     \r\n [5] \"every single pediatrician know was first in line to immunize their children against that should tell you somethin\"                                        \r\n [6] \"children from kupwara kashmir sang song as tribute to indian army on army day this shows indianarmy has succe\"                                            \r\n [7] \"only states and c report vaccination data by race ethnicity for kids including states and c who report this da\"                                           \r\n [8] \"did you know covax has delivered one in ten vaccines worldwide find out more about the partnership in this\"                                               \r\n [9] \"billion doses of vaccines have been delivered to countries across the incl via global vaccine platform\"                                                   \r\n[10] \"being an adult means making hard decisions like canceling trip to rochester to go see your favorite bands your bff because rates are raging adultingblows\"\r\n\r\ncleaning tweets\r\n\r\n\r\nclean <- function (text) {\r\n  str_remove_all(text,\" ?(f|ht)(tp)(s?)(://)(.*)[.|/](.*)\") %>%\r\n                            # Remove mentions\r\n                            str_remove_all(\"@[[:alnum:]_]*\") %>%\r\n                            # Remove hash tags\r\n                            str_remove_all(\"#[[:alnum:]_]+\") %>%\r\n                            # Replace \"&\" character reference with \"and\"\r\n                            str_replace_all(\"&amp;\", \"and\") %>%\r\n                            # Remove punctuation, using a standard character class\r\n                            str_remove_all(\"[[:punct:]]\") %>%\r\n                            # remove digits\r\n                            str_remove_all(\"[[:digit:]]\") %>%\r\n                            # Remove \"RT: \" from beginning of retweets\r\n                            str_remove_all(\"^RT:? \") %>%\r\n                            # Replace any newline characters with a space\r\n                            str_replace_all(\"\\\\\\n|\\\\\\r\", \" \") %>%\r\n                            # remove strings like \"<U+0001F9F5>\"\r\n                            str_remove_all(\"<.*?>\") %>% \r\n                            # Make everything lowercase\r\n                            str_to_lower() %>%\r\n                            # Remove any trailing white space around the text and inside a string\r\n                            str_squish()\r\n}\r\n\r\n\r\n\r\n\r\n\r\ntwitter_data$text <- clean(twitter_data$text)\r\n\r\n\r\n\r\nlooking at the cleaned tweets\r\n\r\n\r\ntwitter_data$text[1:10]\r\n\r\n\r\n [1] \"the dashboard has been updated on january new cases and deaths in days of\"                                                                                \r\n [2] \"health canada has authorized the first treatment in pill form that can be used at home to help prevent ser\"                                               \r\n [3] \"vaccine year of vaccination my gettr\"                                                                                                                     \r\n [4] \"hey unvaccinated people what your excuse now even sheep are doing it\"                                                                                     \r\n [5] \"every single pediatrician know was first in line to immunize their children against that should tell you somethin\"                                        \r\n [6] \"children from kupwara kashmir sang song as tribute to indian army on army day this shows indianarmy has succe\"                                            \r\n [7] \"only states and c report vaccination data by race ethnicity for kids including states and c who report this da\"                                           \r\n [8] \"did you know covax has delivered one in ten vaccines worldwide find out more about the partnership in this\"                                               \r\n [9] \"billion doses of vaccines have been delivered to countries across the incl via global vaccine platform\"                                                   \r\n[10] \"being an adult means making hard decisions like canceling trip to rochester to go see your favorite bands your bff because rates are raging adultingblows\"\r\n\r\nconverting the character vectors, text,\r\nretweet_text to a single corpus\r\n\r\n\r\ntext <- corpus(c(twitter_data$text, twitter_data$retweet_text))\r\ntext <- dfm(tokens(text, remove_punct=TRUE, remove_numbers = TRUE) %>%\r\n             tokens_select(pattern=stopwords(\"en\"),\r\n                            selection=\"remove\"))\r\n\r\n\r\n\r\nfinding the word frequencies in the documents/ tweets\r\n\r\n\r\nword_counts <- as.data.frame(sort(colSums(text),dec=T))\r\ncolnames(word_counts) <- c(\"Frequency\")\r\nword_counts$word <- row.names(word_counts)\r\nword_counts$Rank <- c(1:ncol(text))\r\nhead(word_counts)\r\n\r\n\r\n            Frequency        word Rank\r\ncases          226946       cases    1\r\nnew            210197         new    2\r\npeople         167343      people    3\r\nvaccine        163866     vaccine    4\r\ncoronavirus    149677 coronavirus    5\r\nomicron        137526     omicron    6\r\n\r\ncounting the words that have frequency leass than 50\r\n\r\n\r\nsum(word_counts$Frequency < 50)\r\n\r\n\r\n[1] 147378\r\n\r\nlooking at how word frequencies are distributed\r\n\r\n\r\nggplot(word_counts, mapping = aes(x = Rank, y = Frequency)) + \r\n  geom_point() +\r\n  labs(title = \"Zipf's Law\", x = \"Rank\", y = \"Frequency\") + \r\n  theme_bw()\r\n\r\n\r\n\r\n\r\nHaving seen what we are working with here, we might start to think\r\nthat our matrix still contains too many uninformative or very rare\r\nterms. We can trim our DFM in two different ways related to feature\r\nfrequencies using dfm_trim().\r\n\r\n\r\n# let's create a nicer dfm by limiting to words that appear frequently and are in more than 30% of tweets\r\n#text_dfm <- dfm_trim(text, min_termfreq = 50)\r\ntext_dfm <- dfm_trim(text, min_termfreq = .3, docfreq_type = \"prop\")\r\n\r\n# create fcm from dfm\r\ntext_fcm <- fcm(text_dfm)\r\n\r\n# check the dimensions (i.e., the number of rows and the number of columnns)\r\n# of the matrix we created\r\ndim(text_fcm)\r\n\r\n\r\n[1] 167217 167217\r\n\r\nhead(text_fcm, 10)\r\n\r\n\r\nFeature co-occurrence matrix of: 10 by 167,217 features.\r\n            features\r\nfeatures     dashboard updated january   new  cases deaths days\r\n  dashboard        203    1882     827  1685   3729   2815 1465\r\n  updated            0     113     858  2210   3851   2951 1932\r\n  january            0       0     517  5480   8149   4382 1103\r\n  new                0       0       0 32037 130482  50268 6670\r\n  cases              0       0       0     0  57566  78553 8452\r\n  deaths             0       0       0     0      0  17171 6134\r\n  days               0       0       0     0      0      0 1980\r\n  health             0       0       0     0      0      0    0\r\n  canada             0       0       0     0      0      0    0\r\n  authorized         0       0       0     0      0      0    0\r\n            features\r\nfeatures     health canada authorized\r\n  dashboard     175      5          0\r\n  updated       723    210          6\r\n  january      1022    361          3\r\n  new          8960   1547         44\r\n  cases       17538   1535          7\r\n  deaths      12210   1478          6\r\n  days         1378    725          6\r\n  health      10015   4900        330\r\n  canada          0   6017        358\r\n  authorized      0      0          6\r\n[ reached max_nfeat ... 167,207 more features ]\r\n\r\n\r\n\r\n# pull the top features\r\nmyFeatures <- names(topfeatures(text_fcm, 30))\r\n\r\n# retain only those top features as part of our matrix\r\neven_text_fcm <- fcm_select(text_fcm, pattern = myFeatures, selection = \"keep\")\r\n\r\n# check dimensions\r\ndim(even_text_fcm)\r\n\r\n\r\n[1] 30 30\r\n\r\n# compute size weight for vertices in network\r\nsize <- log(colSums(even_text_fcm))\r\n\r\n# create plot\r\ntextplot_network(even_text_fcm, vertex_size = size / max(size) * 3)\r\n\r\n\r\n\r\n\r\nThe function get_sentiments() allows us to get specific sentiment\r\nlexicons with the appropriate measures for each one. This is a function\r\nin tidytext package\r\nThis dataset nrc was published in Saif M. Mohammad and Peter Turney.\r\n(2013), ``Crowdsourcing a Word-Emotion Association Lexicon.’’\r\nComputational Intelligence, 29(3): 436-465.\r\n\r\n\r\nnrc <- get_sentiments(\"nrc\")\r\n\r\n\r\n\r\nthis dataset afinn was published by Finn Årup Nielsen (http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=6010)\r\n\r\n\r\nafinn <- get_sentiments(\"afinn\")\r\n\r\n\r\n\r\nthis dataset bing was published by Bing Liu and collaborators,https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html\r\n\r\n\r\nbing <- get_sentiments(\"bing\")\r\n\r\n\r\n\r\nsentiment analysis\r\nThe nrc lexicon categorizes words in a binary fashion (“yes”/“no”)\r\ninto categories of positive, negative, anger, anticipation, disgust,\r\nfear, joy, sadness, surprise, and trust.\r\n\r\n\r\n(nrc <- word_counts %>%\r\n  inner_join(nrc, by = \"word\") %>% \r\n  group_by(sentiment) %>% \r\n  summarise(frequency = n()))\r\n\r\n\r\n# A tibble: 10 x 2\r\n   sentiment    frequency\r\n   <chr>            <int>\r\n 1 anger             1182\r\n 2 anticipation       794\r\n 3 disgust            975\r\n 4 fear              1385\r\n 5 joy                632\r\n 6 negative          3062\r\n 7 positive          2133\r\n 8 sadness           1116\r\n 9 surprise           500\r\n10 trust             1152\r\n\r\n  ggplot(nrc, aes(x = sentiment, y = frequency/sum(frequency), fill = sentiment))+\r\n  geom_bar(stat='identity')+\r\n  labs(y = \"percentage of emotion\")+\r\n  theme_classic()\r\n\r\n\r\n\r\n\r\nThe bing lexicon categorizes words in a binary fashion into positive\r\nand negative categories.\r\n\r\n\r\nword_counts %>%\r\n  inner_join(bing, by = \"word\") %>% \r\n  group_by(sentiment) %>% \r\n  summarise(frequency = n()) \r\n\r\n\r\n# A tibble: 2 x 2\r\n  sentiment frequency\r\n  <chr>         <int>\r\n1 negative       3599\r\n2 positive       1538\r\n\r\nThe AFINN lexicon assigns words with a score that runs between -5 and\r\n5, with negative scores indicating negative sentiment and positive\r\nscores indicating positive sentiment.\r\n\r\n\r\nword_counts %>%\r\n  inner_join(afinn, by = \"word\") %>% \r\n  group_by(value) %>% \r\n  summarise(frequency = n()) %>% \r\n  arrange(desc(frequency))\r\n\r\n\r\n# A tibble: 10 x 2\r\n   value frequency\r\n   <dbl>     <int>\r\n 1    -2       874\r\n 2     2       414\r\n 3    -1       293\r\n 4    -3       238\r\n 5     1       204\r\n 6     3       153\r\n 7    -4        39\r\n 8     4        38\r\n 9    -5        15\r\n10     5         5\r\n\r\nmost common positive and negative words\r\n\r\n\r\nhead(bing_word_counts <- word_counts %>%\r\n  inner_join(get_sentiments(\"bing\")) %>%\r\n   select(word, Frequency, sentiment), 30)\r\n\r\n\r\n         word Frequency sentiment\r\n1       virus     58719  negative\r\n2    positive     57913  positive\r\n3        like     50880  positive\r\n4       death     46987  negative\r\n5        free     45752  positive\r\n6        work     36684  positive\r\n7        died     36610  negative\r\n8        risk     35625  negative\r\n9    breaking     33144  negative\r\n10       safe     32545  positive\r\n11       well     30308  positive\r\n12    freedom     30232  positive\r\n13    protect     28426  positive\r\n14       good     28288  positive\r\n15  infection     28176  negative\r\n16   benefits     27731  positive\r\n17   symptoms     26354  negative\r\n18      saint     23853  positive\r\n19      great     23668  positive\r\n20    support     23439  positive\r\n21      right     21834  positive\r\n22     enough     20903  positive\r\n23 protection     20267  positive\r\n24       lost     19854  negative\r\n25     crisis     18816  negative\r\n26     better     18022  positive\r\n27       best     17348  positive\r\n28 infections     17135  negative\r\n29  available     17007  positive\r\n30   recovery     15911  positive\r\n\r\n\r\n\r\nhead(nrc_word_counts <- word_counts %>%\r\n  inner_join(get_sentiments(\"nrc\")) %>%\r\n   filter(sentiment %in% c(\"positve\", \"negative\")) %>% \r\n   select(word, Frequency, sentiment), 30)\r\n\r\n\r\n         word Frequency sentiment\r\n1    pandemic    126603  negative\r\n2       virus     58719  negative\r\n3  government     47388  negative\r\n4       death     46987  negative\r\n5        risk     35625  negative\r\n6   infection     28176  negative\r\n7         war     27816  negative\r\n8        case     26888  negative\r\n9        wear     23451  negative\r\n10      fight     23145  negative\r\n11    disease     20578  negative\r\n12       lost     19854  negative\r\n13     crisis     18816  negative\r\n14     locust     17224  negative\r\n15       sick     14344  negative\r\n16    highest     13871  negative\r\n17  emergency     13376  negative\r\n18       shot     13027  negative\r\n19        flu     12825  negative\r\n20  mortality     12423  negative\r\n21     cancer     11628  negative\r\n22 infectious     11395  negative\r\n23  dangerous     10701  negative\r\n24  isolation     10482  negative\r\n25     threat      9943  negative\r\n26     deadly      9376  negative\r\n27        die      9129  negative\r\n28     excess      8287  negative\r\n29      leave      8214  negative\r\n30       fear      7937  negative\r\n\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-05-13T18:02:42-04:00",
    "input_file": "fourth_post.knit.md"
  },
  {
    "path": "posts/third_post/",
    "title": "Initial Analysis",
    "description": "Initial Understanding of the Data.",
    "author": [
      {
        "name": "Sathvik Thogaru",
        "url": "https://thogarusathvik.github.io/Text-as-Data-Project/"
      }
    ],
    "date": "2022-02-28",
    "categories": [],
    "contents": "\r\n\r\nbody {\r\ntext-align: justify}\r\nThis is just an initial glimpse at the how will the data look like\r\nonce I download using the API. I have created a word cloud using the\r\ninitially collected 200 observations of data( data is not cleaned\r\nproperly) from the Twitter using the twitter API’s and the hashtag used\r\nis #COVID19\r\n\r\n\r\nlibrary(rtweet)\r\nlibrary(quanteda)\r\nlibrary(tidyr)\r\nlibrary(quanteda.textplots)\r\nlibrary(dplyr)\r\n\r\n\r\n\r\nSince the API Keys and Tokens used needs to secured and cannot be\r\nshared I didn’t include my data scrapping code in this post. Below is\r\nthe syntax that I used to get the tweets.\r\nlibrary(rtweet)\r\nmytoken <- create_token(\r\napp = \"scrapetext\",\r\nconsumer_key = \"#############\",\r\nconsumer_secret = \"##############\",\r\naccess_token = \"#############\",\r\naccess_secret = \"#############\")\r\ndata <- search_tweets(\"#Covid\", include_retweets = TRUE,\r\nn = 1000, retryonratelimit = TRUE,  token = mytoken, lang = 'en')\r\nsave(data, file = 'twitter_data.rda')\r\nusing the above R code I extracted the 1000 tweets which\r\nare in English. The retweets are also\r\nincluded\r\npersonal twitter dev account, using twitter API extracted the 200\r\nmixed type of tweets which are in english. The retweets are also\r\nincluded\r\n\r\n\r\nlibrary(readr)\r\ntwitter_data <- read_csv(\"../../twitter_data.csv\")\r\n\r\n\r\n\r\nrange of data available\r\n\r\n\r\nrange(twitter_data$created_at)\r\n\r\n\r\n[1] \"2022-03-24 06:44:16 UTC\" \"2022-03-25 20:57:13 UTC\"\r\n\r\nhastags that are included in the tweets\r\n\r\n\r\nhashtags <- list()\r\nfor (i in twitter_data$hashtags){hashtags <- c(hashtags, i)}\r\n\r\n\r\n\r\nbuilding wordcloud\r\n\r\n\r\nhashtags_c = unlist(hashtags)\r\ntextplot_wordcloud(dfm(corpus(hashtags_c)))\r\n\r\n\r\n\r\n\r\nconverting the character vectors, text,\r\nretweet_text to a single corpus\r\n\r\n\r\ntext <- corpus(c(twitter_data$text, twitter_data$retweet_text))\r\ntext <- tokens(text, remove_punct=TRUE, remove_numbers = TRUE) %>%\r\n             tokens_select(pattern=stopwords(\"en\"),\r\n                            selection=\"remove\")\r\n\r\n\r\n\r\nremoving hashtags removing links\r\n\r\n\r\ntext <- sub(\"(?:\\\\s*#\\\\w+)+\\\\s*$\", \"\", text)\r\ntext <- gsub(\"http.+\", \"\", text) \r\n\r\n\r\n\r\nvisual representation of the tweet and retweet text data that is\r\navailable in the collected data\r\n\r\n\r\ntextplot_wordcloud(dfm(text))\r\n\r\n\r\n\r\n\r\nlooks like there are a lot of other text used in the tweets which\r\nneeds to be cleaned, this may be the reason the words are not really\r\nmeaningful.\r\nNow the following is the code used for downloading tweets from\r\nAcademic twitter API for analysis with more larger data\r\nset_bearer()\r\n(academictwitteR)\r\ntweets <- get_all_tweets( query = c(“#Covid19”,\r\n“#Coronavirus”, “#StayHome”), start_tweets = “2022-01-01T00:00:00Z”,\r\nend_tweets = “2022-02-28T00:00:00Z”, remove_promoted = TRUE, lang =\r\n“en”, file = “scraped_data1_”, data_path = “data1/”, n = 1000000,\r\n)\r\nc <- bind_tweets(data_path = “data/”, output_format =\r\n“tidy”)\r\nwrite.csv(c, “combined_data.csv”)\r\n\r\n\r\n\r\n",
    "preview": "posts/third_post/third_post_files/figure-html5/unnamed-chunk-5-1.png",
    "last_modified": "2022-05-13T17:56:33-04:00",
    "input_file": "third_post.knit.md"
  },
  {
    "path": "posts/second_post/",
    "title": "Second Post: Data Collection",
    "description": "Data collection and Characteristics of Data Source",
    "author": [
      {
        "name": "Sathvik Thogaru",
        "url": "https://thogarusathvik.github.io/Text-as-Data-Project/"
      }
    ],
    "date": "2022-02-21",
    "categories": [],
    "contents": "\r\n\r\nbody {\r\ntext-align: justify}\r\nFor my Research Question which I have stated in my First post, I will\r\ncollect the Tweets of all the possible Covid19\r\nData that I can get. The Data will be collected using the\r\nTwitter API. The Twitter API helps to read and\r\nwrite Twitter data. It is also helpful to\r\ncompose tweets, read profiles, and\r\naccess followers' data and a high volume of tweets on\r\nparticular subjects in specific locations. API stands for\r\nApplication Programming Interface.\r\nTo get access to Twitter API, I will first sign up for\r\nTwitter Developer Account,then get access to my\r\nAPI keys and Tokens. After getting the\r\nrequired Keys and Tokens I make my initial request by using the\r\nsearch_tweets() fuction in package rtweet and\r\nstore the data in a file which I will be using for my research.\r\nThe initial glimpse of the data and its content will be availabe in\r\nthe next post.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-05-13T17:49:38-04:00",
    "input_file": "second_post.knit.md"
  },
  {
    "path": "posts/welcome/",
    "title": "First Post: Introduction",
    "description": "Welcome to my blog Text as data, please click to check initial draft. \n Kindly look for future posts for furthur information. New posts every week :-)...",
    "author": [
      {
        "name": "Sathvik Thogaru",
        "url": "https://thogarusathvik.github.io/Text-as-Data-Project/"
      }
    ],
    "date": "2022-02-18",
    "categories": [],
    "contents": "\r\n\r\nbody {\r\ntext-align: justify}\r\nTwitter, Facebook, Reddit and other social media platforms represent\r\na large and largely untapped resource for social data and evidence.The\r\ndata in these platforms is vast, contains variety of topics that people\r\ndiscuss on the daily basis regardless of geographical location. This\r\ndata that is generated on the daily basis provides a key/main source of\r\ninformation for anyone who wants to study science, communication, people\r\nopinions and situations happening across the world at the same time.\r\nThese platforms provide data through number of applicatyion\r\nProgramming Interfaces(API). Although it may be diificult or sometimes\r\nnot possible to obtain data from these platforms, they help in\r\nqualitative and quantitative research.\r\nCovid-19 outbreak was nothing less than the biggest nightmare the\r\nhumanity has gone through that has changed the lives of millions\r\nupsidedown in no time and almost everyone have suffered. Even with the\r\nresearch advancements we see and develop every single day, yet we still\r\nhaven’t figured out completely to eliminate the virus. So we adapted\r\nourselves to live with it but that didn’t happen overnight. In my study\r\nI am trying to collect the voices of people across all borders what they\r\nshared on social media platforms like Twitter, Reddit etc., and\r\ndifferentiate the data according to the phases of the outbreak while\r\nlisting the most common problems that masses faced at each stage and\r\nestablish the relation on how the trend shifting happened overtime and\r\nwho were most impacted by it.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-03-16T01:35:23-04:00",
    "input_file": "welcome.knit.md"
  }
]
